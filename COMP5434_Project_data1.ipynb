{"cells":[{"cell_type":"markdown","metadata":{"id":"clW5J3cIkqcR"},"source":["# SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5lLUDQTnGNC"},"outputs":[],"source":["vectorizer_T1 = TfidfVectorizer(max_features=1000000)\n","vectorizer_T1.fit(train_set[\"T1\"])\n","train_X_T1 = vectorizer_T1.transform(train_set[\"T1\"])\n","train_Y_T1 = vectorizer_T1.transform(test_set[\"T1\"])\n","\n","vectorizer_T2 = TfidfVectorizer(max_features=1000000)\n","vectorizer_T2.fit(train_set[\"T2\"])\n","train_X_T2 = vectorizer_T2.transform(train_set[\"T2\"])\n","train_Y_T2 = vectorizer_T2.transform(test_set[\"T2\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPIY9V7AdXNf"},"outputs":[],"source":["print(\"For SVM in T1\")\n","start_time = time.time()\n","SVM_T1 = svm.SVC()\n","SVM_T1.fit(train_X_T1,train_set[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T1 = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T1))\n","predictions_SVM_T1 = SVM_T1.predict(train_Y_T1)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_set[\"class label\"],predictions_SVM_T1) * 100))\n","\n","print(\"For NB in T1\")\n","start_time = time.time()\n","Naive_T1 = naive_bayes.MultinomialNB()\n","Naive_T1.fit(train_X_T1,train_set[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T1 = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T1))\n","predictions_NB_T1 = Naive_T1.predict(train_Y_T1)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_set[\"class label\"],predictions_NB_T1) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKeaq1Bpddeg"},"outputs":[],"source":["print(\"For SVM in T2\")\n","start_time = time.time()\n","SVM_T2 = svm.SVC()\n","SVM_T2.fit(train_X_T2,train_set[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T2 = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T2))\n","predictions_SVM_T2 = SVM_T2.predict(train_Y_T2)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_set[\"class label\"],predictions_SVM_T2) * 100))\n","\n","print(\"For NB in T2\")\n","start_time = time.time()\n","Naive_T2 = naive_bayes.MultinomialNB()\n","Naive_T2.fit(train_X_T2,train_set[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T2 = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T2))\n","predictions_NB_T2 = Naive_T2.predict(train_Y_T2)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_set[\"class label\"],predictions_NB_T2) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3oX02vdHVDot"},"outputs":[],"source":["train_set_T3={}\n","train_set_T3[\"T3\"]=[]\n","a=0\n","\n","for text in train_set[\"T1\"]:\n","  train_set_T3[\"T3\"].append(text)\n","\n","for text in train_set[\"T2\"]:\n","  train_set_T3[\"T3\"][a]=train_set_T3[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","test_set_T3={}\n","test_set_T3[\"T3\"]=[]\n","a=0\n","\n","for text in test_set[\"T1\"]:\n","  test_set_T3[\"T3\"].append(text)\n","\n","for text in test_set[\"T2\"]:\n","  test_set_T3[\"T3\"][a]=test_set_T3[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","vectorizer_T3 = TfidfVectorizer(max_features=1000000)\n","vectorizer_T3.fit(train_set_T3[\"T3\"])\n","train_X_T3 = vectorizer_T3.transform(train_set_T3[\"T3\"])\n","train_Y_T3 = vectorizer_T3.transform(test_set_T3[\"T3\"])\n","\n","print(\"For SVM in T3\")\n","start_time = time.time()\n","SVM_T3 = svm.SVC()\n","SVM_T3.fit(train_X_T3,train_set[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T3 = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T3))\n","predictions_SVM_T3 = SVM_T3.predict(train_Y_T3)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_set[\"class label\"],predictions_SVM_T3) * 100))\n","\n","print(\"For NB in T3\")\n","start_time = time.time()\n","Naive_T3 = naive_bayes.MultinomialNB()\n","Naive_T3.fit(train_X_T3,train_set[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T3 = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T3))\n","predictions_NB_T3 = Naive_T3.predict(train_Y_T3)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_set[\"class label\"],predictions_NB_T3) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMjbHBfodSP7"},"outputs":[],"source":["train_obama=train_set[train_set[\"TO\"]==\"obama\"]\n","test_obama=test_set[test_set[\"TO\"]==\"obama\"]\n","vectorizer_T1_obama = TfidfVectorizer(max_features=1000000)\n","vectorizer_T1_obama.fit(train_obama[\"T1\"])\n","train_X_T1_obama = vectorizer_T1_obama.transform(train_obama[\"T1\"])\n","train_Y_T1_obama = vectorizer_T1_obama.transform(test_obama[\"T1\"])\n","\n","print(\"For SVM in T1_obama\")\n","start_time = time.time()\n","SVM_T1_obama = svm.SVC()\n","SVM_T1_obama.fit(train_X_T1_obama,train_obama[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T1_obama = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T1_obama))\n","predictions_SVM_T1_obama = SVM_T1_obama.predict(train_Y_T1_obama)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_obama[\"class label\"],predictions_SVM_T1_obama) * 100))\n","\n","print(\"For NB in T1_obama\")\n","start_time = time.time()\n","Naive_T1_obama = naive_bayes.MultinomialNB()\n","Naive_T1_obama.fit(train_X_T1_obama,train_obama[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T1_obama = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T1_obama))\n","predictions_NB_T1_obama = Naive_T1_obama.predict(train_Y_T1_obama)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_obama[\"class label\"],predictions_NB_T1_obama) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMEaONiti3Xi"},"outputs":[],"source":["train_microsoft=train_set[train_set[\"TO\"]==\"microsoft\"]\n","test_microsoft=test_set[test_set[\"TO\"]==\"microsoft\"]\n","vectorizer_T1_microsoft = TfidfVectorizer(max_features=1000000)\n","vectorizer_T1_microsoft.fit(train_microsoft[\"T1\"])\n","train_X_T1_microsoft = vectorizer_T1_microsoft.transform(train_microsoft[\"T1\"])\n","train_Y_T1_microsoft = vectorizer_T1_microsoft.transform(test_microsoft[\"T1\"])\n","\n","print(\"For SVM in T1_microsoft\")\n","start_time = time.time()\n","SVM_T1_microsoft = svm.SVC()\n","SVM_T1_microsoft.fit(train_X_T1_microsoft,train_microsoft[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T1_microsoft = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T1_microsoft))\n","predictions_SVM_T1_microsoft = SVM_T1_microsoft.predict(train_Y_T1_microsoft)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_microsoft[\"class label\"],predictions_SVM_T1_microsoft) * 100))\n","\n","print(\"For NB in T1_microsoft\")\n","start_time = time.time()\n","Naive_T1_microsoft = naive_bayes.MultinomialNB()\n","Naive_T1_microsoft.fit(train_X_T1_microsoft,train_microsoft[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T1_microsoft = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T1_microsoft))\n","predictions_NB_T1_microsoft = Naive_T1_microsoft.predict(train_Y_T1_microsoft)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_microsoft[\"class label\"],predictions_NB_T1_microsoft) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8jKXDE2kOap"},"outputs":[],"source":["train_palestine=train_set[train_set[\"TO\"]==\"palestine\"]\n","test_palestine=test_set[test_set[\"TO\"]==\"palestine\"]\n","vectorizer_T1_palestine = TfidfVectorizer(max_features=1000000)\n","vectorizer_T1_palestine.fit(train_palestine[\"T1\"])\n","train_X_T1_palestine = vectorizer_T1_palestine.transform(train_palestine[\"T1\"])\n","train_Y_T1_palestine = vectorizer_T1_palestine.transform(test_palestine[\"T1\"])\n","\n","print(\"For SVM in T1_palestine\")\n","start_time = time.time()\n","SVM_T1_palestine = svm.SVC()\n","SVM_T1_palestine.fit(train_X_T1_palestine,train_palestine[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T1_palestine = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T1_palestine))\n","predictions_SVM_T1_palestine = SVM_T1_palestine.predict(train_Y_T1_palestine)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_palestine[\"class label\"],predictions_SVM_T1_palestine) * 100))\n","\n","print(\"For NB in T1_palestine\")\n","start_time = time.time()\n","Naive_T1_palestine = naive_bayes.MultinomialNB()\n","Naive_T1_palestine.fit(train_X_T1_palestine,train_palestine[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T1_palestine = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T1_palestine))\n","predictions_NB_T1_palestine = Naive_T1_palestine.predict(train_Y_T1_palestine)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_palestine[\"class label\"],predictions_NB_T1_palestine) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6E_TxYYngJu"},"outputs":[],"source":["train_economy=train_set[train_set[\"TO\"]==\"economy\"]\n","test_economy=test_set[test_set[\"TO\"]==\"economy\"]\n","vectorizer_T1_economy = TfidfVectorizer(max_features=1000000)\n","vectorizer_T1_economy.fit(train_economy[\"T1\"])\n","train_X_T1_economy = vectorizer_T1_economy.transform(train_economy[\"T1\"])\n","train_Y_T1_economy = vectorizer_T1_economy.transform(test_economy[\"T1\"])\n","\n","print(\"For SVM in T1_economy\")\n","start_time = time.time()\n","SVM_T1_economy = svm.SVC()\n","SVM_T1_economy.fit(train_X_T1_economy,train_economy[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T1_economy = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T1_economy))\n","predictions_SVM_T1_economy = SVM_T1_economy.predict(train_Y_T1_economy)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_economy[\"class label\"],predictions_SVM_T1_economy) * 100))\n","\n","print(\"For NB in T1_economy\")\n","start_time = time.time()\n","Naive_T1_economy = naive_bayes.MultinomialNB()\n","Naive_T1_economy.fit(train_X_T1_economy,train_economy[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T1_economy = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T1_economy))\n","predictions_NB_T1_economy = Naive_T1_economy.predict(train_Y_T1_economy)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_economy[\"class label\"],predictions_NB_T1_economy) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hc9TdP9PsB2X"},"outputs":[],"source":["train_obama=train_set[train_set[\"TO\"]==\"obama\"]\n","test_obama=test_set[test_set[\"TO\"]==\"obama\"]\n","vectorizer_T2_obama = TfidfVectorizer(max_features=1000000)\n","vectorizer_T2_obama.fit(train_obama[\"T2\"])\n","train_X_T2_obama = vectorizer_T2_obama.transform(train_obama[\"T2\"])\n","train_Y_T2_obama = vectorizer_T2_obama.transform(test_obama[\"T2\"])\n","\n","print(\"For SVM in T2_obama\")\n","start_time = time.time()\n","SVM_T2_obama = svm.SVC()\n","SVM_T2_obama.fit(train_X_T2_obama,train_obama[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T2_obama = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T2_obama))\n","predictions_SVM_T2_obama = SVM_T2_obama.predict(train_Y_T2_obama)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_obama[\"class label\"],predictions_SVM_T2_obama) * 100))\n","\n","print(\"For NB in T2_obama\")\n","start_time = time.time()\n","Naive_T2_obama = naive_bayes.MultinomialNB()\n","Naive_T2_obama.fit(train_X_T2_obama,train_obama[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T2_obama = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T2_obama))\n","predictions_NB_T2_obama = Naive_T2_obama.predict(train_Y_T2_obama)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_obama[\"class label\"],predictions_NB_T2_obama) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Dcs1bXHs2E1"},"outputs":[],"source":["train_microsoft=train_set[train_set[\"TO\"]==\"microsoft\"]\n","test_microsoft=test_set[test_set[\"TO\"]==\"microsoft\"]\n","vectorizer_T2_microsoft = TfidfVectorizer(max_features=1000000)\n","vectorizer_T2_microsoft.fit(train_microsoft[\"T2\"])\n","train_X_T2_microsoft = vectorizer_T2_microsoft.transform(train_microsoft[\"T2\"])\n","train_Y_T2_microsoft = vectorizer_T2_microsoft.transform(test_microsoft[\"T2\"])\n","\n","print(\"For SVM in T2_microsoft\")\n","start_time = time.time()\n","SVM_T2_microsoft = svm.SVC()\n","SVM_T2_microsoft.fit(train_X_T2_microsoft,train_microsoft[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T2_microsoft = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T2_microsoft))\n","predictions_SVM_T2_microsoft = SVM_T2_microsoft.predict(train_Y_T2_microsoft)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_microsoft[\"class label\"],predictions_SVM_T2_microsoft) * 100))\n","\n","print(\"For NB in T2_microsoft\")\n","start_time = time.time()\n","Naive_T2_microsoft = naive_bayes.MultinomialNB()\n","Naive_T2_microsoft.fit(train_X_T2_microsoft,train_microsoft[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T2_microsoft = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T2_microsoft))\n","predictions_NB_T2_microsoft = Naive_T2_microsoft.predict(train_Y_T2_microsoft)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_microsoft[\"class label\"],predictions_NB_T2_microsoft) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESRv9vEutE0s"},"outputs":[],"source":["train_palestine=train_set[train_set[\"TO\"]==\"palestine\"]\n","test_palestine=test_set[test_set[\"TO\"]==\"palestine\"]\n","vectorizer_T2_palestine = TfidfVectorizer(max_features=1000000)\n","vectorizer_T2_palestine.fit(train_palestine[\"T2\"])\n","train_X_T2_palestine = vectorizer_T2_palestine.transform(train_palestine[\"T2\"])\n","train_Y_T2_palestine = vectorizer_T2_palestine.transform(test_palestine[\"T2\"])\n","\n","print(\"For SVM in T2_palestine\")\n","start_time = time.time()\n","SVM_T2_palestine = svm.SVC()\n","SVM_T2_palestine.fit(train_X_T2_palestine,train_palestine[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T2_palestine = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T2_palestine))\n","predictions_SVM_T2_palestine = SVM_T2_palestine.predict(train_Y_T2_palestine)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_palestine[\"class label\"],predictions_SVM_T2_palestine) * 100))\n","\n","print(\"For NB in T2_palestine\")\n","start_time = time.time()\n","Naive_T2_palestine = naive_bayes.MultinomialNB()\n","Naive_T2_palestine.fit(train_X_T2_palestine,train_palestine[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T2_palestine = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T2_palestine))\n","predictions_NB_T2_palestine = Naive_T2_palestine.predict(train_Y_T2_palestine)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_palestine[\"class label\"],predictions_NB_T2_palestine) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cULHaKptPcX"},"outputs":[],"source":["train_economy=train_set[train_set[\"TO\"]==\"economy\"]\n","test_economy=test_set[test_set[\"TO\"]==\"economy\"]\n","vectorizer_T2_economy = TfidfVectorizer(max_features=1000000)\n","vectorizer_T2_economy.fit(train_economy[\"T2\"])\n","train_X_T2_economy = vectorizer_T2_economy.transform(train_economy[\"T2\"])\n","train_Y_T2_economy = vectorizer_T2_economy.transform(test_economy[\"T2\"])\n","\n","print(\"For SVM in T2_economy\")\n","start_time = time.time()\n","SVM_T2_economy = svm.SVC()\n","SVM_T2_economy.fit(train_X_T2_economy,train_economy[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T2_economy = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T2_economy))\n","predictions_SVM_T2_economy = SVM_T2_economy.predict(train_Y_T2_economy)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_economy[\"class label\"],predictions_SVM_T2_economy) * 100))\n","\n","print(\"For NB in T2_economy\")\n","start_time = time.time()\n","Naive_T2_economy = naive_bayes.MultinomialNB()\n","Naive_T2_economy.fit(train_X_T2_economy,train_economy[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T2_economy = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T2_economy))\n","predictions_NB_T2_economy = Naive_T2_economy.predict(train_Y_T2_economy)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_economy[\"class label\"],predictions_NB_T2_economy) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61wHkcP2uKgY"},"outputs":[],"source":["train_obama=train_set[train_set[\"TO\"]==\"obama\"]\n","test_obama=test_set[test_set[\"TO\"]==\"obama\"]\n","\n","train_set_T3_obama={}\n","train_set_T3_obama[\"T3\"]=[]\n","a=0\n","\n","for text in train_obama[\"T1\"]:\n","  train_set_T3_obama[\"T3\"].append(text)\n","\n","for text in train_obama[\"T2\"]:\n","  train_set_T3_obama[\"T3\"][a]=train_set_T3_obama[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","test_set_T3_obama={}\n","test_set_T3_obama[\"T3\"]=[]\n","a=0\n","\n","for text in test_obama[\"T1\"]:\n","  test_set_T3_obama[\"T3\"].append(text)\n","\n","for text in test_obama[\"T2\"]:\n","  test_set_T3_obama[\"T3\"][a]=test_set_T3_obama[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","vectorizer_T3_obama = TfidfVectorizer(max_features=1000000)\n","vectorizer_T3_obama.fit(train_set_T3_obama[\"T3\"])\n","train_X_T3_obama = vectorizer_T3_obama.transform(train_set_T3_obama[\"T3\"])\n","train_Y_T3_obama = vectorizer_T3_obama.transform(test_set_T3_obama[\"T3\"])\n","\n","print(\"For SVM in T3_obama\")\n","start_time = time.time()\n","SVM_T3_obama = svm.SVC()\n","SVM_T3_obama.fit(train_X_T3_obama,train_obama[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T3_obama = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T3_obama))\n","predictions_SVM_T3_obama = SVM_T3_obama.predict(train_Y_T3_obama)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_obama[\"class label\"],predictions_SVM_T3_obama) * 100))\n","\n","print(\"For NB in T3_obama\")\n","start_time = time.time()\n","Naive_T3_obama = naive_bayes.MultinomialNB()\n","Naive_T3_obama.fit(train_X_T3_obama,train_obama[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T3_obama = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T3_obama))\n","predictions_NB_T3_obama = Naive_T3_obama.predict(train_Y_T3_obama)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_obama[\"class label\"],predictions_NB_T3_obama) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyF1Nx_jwdOT"},"outputs":[],"source":["train_microsoft=train_set[train_set[\"TO\"]==\"microsoft\"]\n","test_microsoft=test_set[test_set[\"TO\"]==\"microsoft\"]\n","\n","train_set_T3_microsoft={}\n","train_set_T3_microsoft[\"T3\"]=[]\n","a=0\n","\n","for text in train_microsoft[\"T1\"]:\n","  train_set_T3_microsoft[\"T3\"].append(text)\n","\n","for text in train_microsoft[\"T2\"]:\n","  train_set_T3_microsoft[\"T3\"][a]=train_set_T3_microsoft[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","test_set_T3_microsoft={}\n","test_set_T3_microsoft[\"T3\"]=[]\n","a=0\n","\n","for text in test_microsoft[\"T1\"]:\n","  test_set_T3_microsoft[\"T3\"].append(text)\n","\n","for text in test_microsoft[\"T2\"]:\n","  test_set_T3_microsoft[\"T3\"][a]=test_set_T3_microsoft[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","vectorizer_T3_microsoft = TfidfVectorizer(max_features=1000000)\n","vectorizer_T3_microsoft.fit(train_set_T3_microsoft[\"T3\"])\n","train_X_T3_microsoft = vectorizer_T3_microsoft.transform(train_set_T3_microsoft[\"T3\"])\n","train_Y_T3_microsoft = vectorizer_T3_microsoft.transform(test_set_T3_microsoft[\"T3\"])\n","\n","print(\"For SVM in T3_microsoft\")\n","start_time = time.time()\n","SVM_T3_microsoft = svm.SVC()\n","SVM_T3_microsoft.fit(train_X_T3_microsoft,train_microsoft[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T3_microsoft = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T3_microsoft))\n","predictions_SVM_T3_microsoft = SVM_T3_microsoft.predict(train_Y_T3_microsoft)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_microsoft[\"class label\"],predictions_SVM_T3_microsoft) * 100))\n","\n","print(\"For NB in T3_microsoft\")\n","start_time = time.time()\n","Naive_T3_microsoft = naive_bayes.MultinomialNB()\n","Naive_T3_microsoft.fit(train_X_T3_microsoft,train_microsoft[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T3_microsoft = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T3_microsoft))\n","predictions_NB_T3_microsoft = Naive_T3_microsoft.predict(train_Y_T3_microsoft)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_microsoft[\"class label\"],predictions_NB_T3_microsoft) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8ibF9qnwg33"},"outputs":[],"source":["train_palestine=train_set[train_set[\"TO\"]==\"palestine\"]\n","test_palestine=test_set[test_set[\"TO\"]==\"palestine\"]\n","\n","train_set_T3_palestine={}\n","train_set_T3_palestine[\"T3\"]=[]\n","a=0\n","\n","for text in train_palestine[\"T1\"]:\n","  train_set_T3_palestine[\"T3\"].append(text)\n","\n","for text in train_palestine[\"T2\"]:\n","  train_set_T3_palestine[\"T3\"][a]=train_set_T3_palestine[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","test_set_T3_palestine={}\n","test_set_T3_palestine[\"T3\"]=[]\n","a=0\n","\n","for text in test_palestine[\"T1\"]:\n","  test_set_T3_palestine[\"T3\"].append(text)\n","\n","for text in test_palestine[\"T2\"]:\n","  test_set_T3_palestine[\"T3\"][a]=test_set_T3_palestine[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","vectorizer_T3_palestine = TfidfVectorizer(max_features=1000000)\n","vectorizer_T3_palestine.fit(train_set_T3_palestine[\"T3\"])\n","train_X_T3_palestine = vectorizer_T3_palestine.transform(train_set_T3_palestine[\"T3\"])\n","train_Y_T3_palestine = vectorizer_T3_palestine.transform(test_set_T3_palestine[\"T3\"])\n","\n","print(\"For SVM in T3_palestine\")\n","start_time = time.time()\n","SVM_T3_palestine = svm.SVC()\n","SVM_T3_palestine.fit(train_X_T3_palestine,train_palestine[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T3_palestine = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T3_palestine))\n","predictions_SVM_T3_palestine = SVM_T3_palestine.predict(train_Y_T3_palestine)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_palestine[\"class label\"],predictions_SVM_T3_palestine) * 100))\n","\n","print(\"For NB in T3_palestine\")\n","start_time = time.time()\n","Naive_T3_palestine = naive_bayes.MultinomialNB()\n","Naive_T3_palestine.fit(train_X_T3_palestine,train_palestine[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T3_palestine = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T3_palestine))\n","predictions_NB_T3_palestine = Naive_T3_palestine.predict(train_Y_T3_palestine)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_palestine[\"class label\"],predictions_NB_T3_palestine) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95cvMhyjwo0A"},"outputs":[],"source":["train_economy=train_set[train_set[\"TO\"]==\"economy\"]\n","test_economy=test_set[test_set[\"TO\"]==\"economy\"]\n","\n","train_set_T3_economy={}\n","train_set_T3_economy[\"T3\"]=[]\n","a=0\n","\n","for text in train_economy[\"T1\"]:\n","  train_set_T3_economy[\"T3\"].append(text)\n","\n","for text in train_economy[\"T2\"]:\n","  train_set_T3_economy[\"T3\"][a]=train_set_T3_economy[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","test_set_T3_economy={}\n","test_set_T3_economy[\"T3\"]=[]\n","a=0\n","\n","for text in test_economy[\"T1\"]:\n","  test_set_T3_economy[\"T3\"].append(text)\n","\n","for text in test_economy[\"T2\"]:\n","  test_set_T3_economy[\"T3\"][a]=test_set_T3_economy[\"T3\"][a]+\" \"+text\n","  a+=1\n","\n","vectorizer_T3_economy = TfidfVectorizer(max_features=1000000)\n","vectorizer_T3_economy.fit(train_set_T3_economy[\"T3\"])\n","train_X_T3_economy = vectorizer_T3_economy.transform(train_set_T3_economy[\"T3\"])\n","train_Y_T3_economy = vectorizer_T3_economy.transform(test_set_T3_economy[\"T3\"])\n","\n","print(\"For SVM in T3_economy\")\n","start_time = time.time()\n","SVM_T3_economy = svm.SVC()\n","SVM_T3_economy.fit(train_X_T3_economy,train_economy[\"class label\"])\n","end_time = time.time()\n","process_time_SVM_T3_economy = round(end_time-start_time,2)\n","print(\"Fitting SVC took {} seconds\".format(process_time_SVM_T3_economy))\n","predictions_SVM_T3_economy = SVM_T3_economy.predict(train_Y_T3_economy)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_economy[\"class label\"],predictions_SVM_T3_economy) * 100))\n","\n","print(\"For NB in T3_economy\")\n","start_time = time.time()\n","Naive_T3_economy = naive_bayes.MultinomialNB()\n","Naive_T3_economy.fit(train_X_T3_economy,train_economy[\"class label\"])\n","end_time = time.time()\n","process_time_NB_T3_economy = round(end_time-start_time,2)\n","print(\"Fitting NB took {} seconds\".format(process_time_NB_T3_economy))\n","predictions_NB_T3_economy = Naive_T3_economy.predict(train_Y_T3_economy)\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_economy[\"class label\"],predictions_NB_T3_economy) * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"051hQX2Hxh4d"},"outputs":[],"source":["#predictList=[predictions_SVM_T1,predictions_NB_T1,predictions_SVM_T2,predictions_NB_T2,predictions_SVM_T3,predictions_NB_T3]\n","\n","predictList=[predictions_SVM_T1,predictions_SVM_T2,predictions_SVM_T3]\n","\n","predictions=[]\n","final=[]\n","\n","count=0\n","count_o=0\n","count_m=0\n","count_p=0\n","count_e=0\n","\n","for tag in test_set[\"TO\"]:\n","  temp=[]\n","  for pred in predictList:\n","    temp.append(pred[count])\n","  count+=1\n","  if tag == \"obama\":\n","    temp.append(predictions_SVM_T1_obama[count_o])\n","    temp.append(predictions_SVM_T2_obama[count_o])\n","    temp.append(predictions_SVM_T3_obama[count_o])\n","    #temp.append(predictions_NB_T1_obama[count_o])\n","    #temp.append(predictions_NB_T2_obama[count_o])\n","    #temp.append(predictions_NB_T3_obama[count_o])\n","    count_o+=1\n","  elif tag == \"microsoft\":\n","    temp.append(predictions_SVM_T1_microsoft[count_m])\n","    temp.append(predictions_SVM_T2_microsoft[count_m])\n","    temp.append(predictions_SVM_T3_microsoft[count_m])\n","    #temp.append(predictions_NB_T1_microsoft[count_m])\n","    #temp.append(predictions_NB_T2_microsoft[count_m])\n","    #temp.append(predictions_NB_T3_microsoft[count_m])\n","    count_m+=1\n","  elif tag == \"palestine\":\n","    temp.append(predictions_SVM_T1_palestine[count_p])\n","    temp.append(predictions_SVM_T2_palestine[count_p])\n","    temp.append(predictions_SVM_T3_palestine[count_p])\n","    #temp.append(predictions_NB_T1_palestine[count_p])\n","    #temp.append(predictions_NB_T2_palestine[count_p])\n","    #temp.append(predictions_NB_T3_palestine[count_p])\n","    count_p+=1\n","  else:\n","    temp.append(predictions_SVM_T1_economy[count_e])\n","    temp.append(predictions_SVM_T2_economy[count_e])\n","    temp.append(predictions_SVM_T3_economy[count_e])\n","    #temp.append(predictions_NB_T1_economy[count_e])\n","    #temp.append(predictions_NB_T2_economy[count_e])\n","    #temp.append(predictions_NB_T3_economy[count_e])\n","    count_e+=1\n","  predictions.append(temp)\n","\n","for prediction in predictions:\n","  max = 0\n","  res = prediction[0]\n","  for i in prediction:\n","    freq = prediction.count(i)\n","    if freq > max:\n","      max = freq\n","      res = i\n","  final.append(res)\n","\n","print(\"Accuracy of model is {}%\".format(accuracy_score(test_set[\"class label\"],final) * 100))"]},{"cell_type":"markdown","metadata":{"id":"mvn5f4A5kla6"},"source":["# Import"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29835,"status":"ok","timestamp":1699985562961,"user":{"displayName":"nishino nanase","userId":"03566489110485021843"},"user_tz":-480},"id":"j1ugblV8o7oH","outputId":"16278171-2f07-4770-e71c-b89d5a62e400"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n","2023-11-14 18:12:22.738406: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-11-14 18:12:22.738511: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-11-14 18:12:22.738627: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-11-14 18:12:24.968767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-md==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_md')\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["!pip install spacy\n","!python -m spacy download en_core_web_md\n","\n","import en_core_web_md\n","nlp = en_core_web_md.load()\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pickle\n","import time\n","import re\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from sklearn import preprocessing, naive_bayes, svm\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score,f1_score\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","metadata":{"id":"vm8X8g8tk4ug"},"source":["# Testing"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":21469,"status":"ok","timestamp":1699985584413,"user":{"displayName":"nishino nanase","userId":"03566489110485021843"},"user_tz":-480},"id":"Jsm81HohqCoL"},"outputs":[],"source":["train_set = pd.read_csv('training.csv')\n","test_set = pd.read_csv('validation.csv')\n","unrelevant_features = [\"id\"]\n","train_set.drop(unrelevant_features,inplace=True,axis=1)\n","test_set.drop(unrelevant_features,inplace=True,axis=1)\n","train_set.dropna(inplace=True)\n","test_set.dropna(inplace=True)\n","\n","lemma = WordNetLemmatizer()\n","swords = stopwords.words(\"english\")\n","\n","train_set[\"T1\"]=[str(text) for text in train_set[\"T1\"]]\n","train_set[\"T1\"]=[re.sub(\"[^a-zA-Z0-9]\",\" \",text) for text in train_set[\"T1\"]]\n","train_set[\"T1\"]=[nltk.word_tokenize(text.lower()) for text in train_set[\"T1\"]]\n","train_set[\"T1\"]=[[lemma.lemmatize(word) for word in text]  for text in train_set[\"T1\"]]\n","train_set[\"T1\"]=[[word for word in text if word not in swords] for text in train_set[\"T1\"]]\n","train_set[\"T1\"]=[\" \".join(text) for text in train_set[\"T1\"]]\n","\n","test_set[\"T1\"]=[str(text) for text in test_set[\"T1\"]]\n","test_set[\"T1\"]=[re.sub(\"[^a-zA-Z0-9]\",\" \",text) for text in test_set[\"T1\"]]\n","test_set[\"T1\"]=[nltk.word_tokenize(text.lower()) for text in test_set[\"T1\"]]\n","test_set[\"T1\"]=[[lemma.lemmatize(word) for word in text]  for text in test_set[\"T1\"]]\n","test_set[\"T1\"]=[[word for word in text if word not in swords] for text in test_set[\"T1\"]]\n","test_set[\"T1\"]=[\" \".join(text) for text in test_set[\"T1\"]]\n","\n","train_set[\"T2\"]=[str(text) for text in train_set[\"T2\"]]\n","train_set[\"T2\"]=[re.sub(\"[^a-zA-Z0-9]\",\" \",text) for text in train_set[\"T2\"]]\n","train_set[\"T2\"]=[nltk.word_tokenize(text.lower()) for text in train_set[\"T2\"]]\n","train_set[\"T2\"]=[[lemma.lemmatize(word) for word in text]  for text in train_set[\"T2\"]]\n","train_set[\"T2\"]=[[word for word in text if word not in swords] for text in train_set[\"T2\"]]\n","train_set[\"T2\"]=[\" \".join(text) for text in train_set[\"T2\"]]\n","\n","test_set[\"T2\"]=[str(text) for text in test_set[\"T2\"]]\n","test_set[\"T2\"]=[re.sub(\"[^a-zA-Z0-9]\",\" \",text) for text in test_set[\"T2\"]]\n","test_set[\"T2\"]=[nltk.word_tokenize(text.lower()) for text in test_set[\"T2\"]]\n","test_set[\"T2\"]=[[lemma.lemmatize(word) for word in text]  for text in test_set[\"T2\"]]\n","test_set[\"T2\"]=[[word for word in text if word not in swords] for text in test_set[\"T2\"]]\n","test_set[\"T2\"]=[\" \".join(text) for text in test_set[\"T2\"]]"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4i4FSQ48k7wy","executionInfo":{"status":"ok","timestamp":1699987485944,"user_tz":-480,"elapsed":1901544,"user":{"displayName":"nishino nanase","userId":"03566489110485021843"}},"outputId":"332b0950-93fd-47ea-bb8d-b9c0f467ae38"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 29993 entries, 0 to 29999\n","Data columns (total 15 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   T1           29993 non-null  object \n"," 1   T2           29993 non-null  object \n"," 2   S            29993 non-null  object \n"," 3   TO           29993 non-null  int64  \n"," 4   S1           29993 non-null  float64\n"," 5   S2           29993 non-null  float64\n"," 6   class label  29993 non-null  float64\n"," 7   scores_T1    29993 non-null  object \n"," 8   scores_T2    29993 non-null  object \n"," 9   compound_T1  29993 non-null  float64\n"," 10  pos_T1       29993 non-null  float64\n"," 11  neg_T1       29993 non-null  float64\n"," 12  compound_T2  29993 non-null  float64\n"," 13  pos_T2       29993 non-null  float64\n"," 14  neg_T2       29993 non-null  float64\n","dtypes: float64(9), int64(1), object(5)\n","memory usage: 3.7+ MB\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 9998 entries, 0 to 9999\n","Data columns (total 15 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   T1           9998 non-null   object \n"," 1   T2           9998 non-null   object \n"," 2   S            9998 non-null   object \n"," 3   TO           9998 non-null   int64  \n"," 4   S1           9998 non-null   float64\n"," 5   S2           9998 non-null   float64\n"," 6   class label  9998 non-null   int64  \n"," 7   scores_T1    9998 non-null   object \n"," 8   scores_T2    9998 non-null   object \n"," 9   compound_T1  9998 non-null   float64\n"," 10  pos_T1       9998 non-null   float64\n"," 11  neg_T1       9998 non-null   float64\n"," 12  compound_T2  9998 non-null   float64\n"," 13  pos_T2       9998 non-null   float64\n"," 14  neg_T2       9998 non-null   float64\n","dtypes: float64(8), int64(2), object(5)\n","memory usage: 1.2+ MB\n"]}],"source":["sia= SentimentIntensityAnalyzer()\n","\n","data_set = [train_set, test_set]\n","text_set = [\"T1\",\"T2\",\"S\"]\n","\n","for df in data_set:\n","  df['scores_T1']=df['T1'].apply(lambda body: sia.polarity_scores(str(body)))\n","  df['scores_T2']=df['T2'].apply(lambda body: sia.polarity_scores(str(body)))\n","  df['compound_T1']=df['scores_T1'].apply(lambda score_dict:score_dict['compound'])\n","  df['pos_T1']=df['scores_T1'].apply(lambda pos_dict:pos_dict['pos'])\n","  df['neg_T1']=df['scores_T1'].apply(lambda neg_dict:neg_dict['neg'])\n","  df['compound_T2']=df['scores_T2'].apply(lambda score_dict:score_dict['compound'])\n","  df['pos_T2']=df['scores_T2'].apply(lambda pos_dict:pos_dict['pos'])\n","  df['neg_T2']=df['scores_T2'].apply(lambda neg_dict:neg_dict['neg'])\n","  for i in range(len(df)):\n","    for text in text_set:\n","      df[text].iloc[i]=nlp(df[text].iloc[i]).similarity(nlp(df[\"TO\"].iloc[i]))\n","  df['TO'].replace(('obama','microsoft','economy','palestine'), (0,1,2,3), inplace=True)\n","  df['class label'].replace((1,2,3,4,5),(0,1,2,3,4), inplace=True)\n","  df.info()"]},{"cell_type":"code","source":["ytrain = train_set['class label']\n","yvalid = test_set['class label']\n","\n","xtrain = train_set.drop(['scores_T1','scores_T2','class label'], axis=1)\n","xvalid = test_set.drop(['scores_T1','scores_T2','class label'], axis=1)\n","xtrain = np.asarray(xtrain).astype('float32')\n","xvalid = np.asarray(xvalid).astype('float32')\n","\n","model = keras.models.Sequential([\n","    keras.layers.Dense(10000, activation=\"relu\"),\n","    keras.layers.Dense(1000, activation=\"relu\"),\n","    keras.layers.Dense(100, activation=\"relu\"),\n","    keras.layers.Dense(5, activation=\"softmax\")\n","])\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=\"adam\",\n","              metrics=[\"accuracy\"])\n","\n","history = model.fit(xtrain, ytrain, epochs=100, batch_size=64, validation_data=(xvalid, yvalid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":247},"id":"QbeaxV2anftg","executionInfo":{"status":"error","timestamp":1699987485945,"user_tz":-480,"elapsed":16,"user":{"displayName":"nishino nanase","userId":"03566489110485021843"}},"outputId":"0e71754b-7324-4e00-fde2-076546e04201"},"execution_count":56,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-c6701504783b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mxvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXLrhQPnEUwHqReuzX3kaj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}